# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/114QBDy1JCMh97syN_F7DhAahhXzmUHXC
"""

!pip freeze > /content/drive/MyDrive/requirements.txt

!pip install -r /content/drive/MyDrive/requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %pip install \
    numpy scipy pandas matplotlib scikit-learn \
    scikit-tda ripser persim gudhi giotto-tda dionysus

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --force-reinstall numpy pandas

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --force-reinstall \
    numpy scipy scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %pip install --upgrade --force-reinstall \
    "numpy<2.0" \
    numba \
    scanpy anndata h5py

import os
os.kill(os.getpid(), 9)

import os
import argparse
import logging
from pathlib import Path
from scipy import sparse
import scanpy as sc
import numpy as np
from ripser import ripser
from persim import plot_diagrams, PersistenceImager
import joblib
import matplotlib.pyplot as plt
from persim import PersistenceImager
from scipy.spatial.distance import pdist, squareform
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster
from sklearn.decomposition import PCA
from persim import bottleneck
import scipy.cluster.hierarchy as sch
from itertools import combinations
from tqdm.auto import tqdm
import pandas as pd


import scipy as sp
from sklearn import preprocessing, manifold, datasets
from google.colab import drive
from typing import Dict

drive.mount('/content/drive')
adata = sc.read_h5ad('/content/drive/MyDrive/tabula_muris-scvr_prep.h5ad')
print(adata)

sc.pp.pca(adata, n_comps=30)
X = adata.obsm['X_pca'][:, :5]
print (X)

def load_ann_data(path: str):
  adata = sc.read_h5ad(path)
  print(f"{adata.n_obs} cells × {adata.n_vars} genes")
  return adata

def inspect_adata(adata):
  print(adata.obs.columns.tolist())
  print(adata.var_names[:10].tolist())

  X = adata.X
  if sparse.issparse(X):
    X_data = X.A
  else:
    X_data = X
  total_entries = X_data.size
  missing = np.isnan(X_data).sum()
  nonzeros = np.count_nonzero(X_data)
  sparsity = 1.0 - nonzeros/total_entries

  print(f"{missing} / {total_entries}")
  print(f"{sparsity:.3%}")

adata = load_ann_data('/content/drive/MyDrive/tabula_muris-scvr_prep.h5ad')
inspect_adata(adata)

def extract_subpopulations(adata, groupby: str = 'cell_ontology_class') -> Dict[str, np.ndarray]:
  expr_dict = {}
  for ct in adata.obs[groupby].unique():
    mask = adata.obs[groupby] == ct
    X = adata[mask].X
    expr_dict[ct] = X.A if sparse.issparse(X) else X
  return expr_dict

expr_dict = extract_subpopulations(adata, 'cell_ontology_class')
print(f"Found {len(expr_dict)} groups:", list(expr_dict)[:5])

def corr2dist(X: np.ndarray,dim_red: bool = True,variance: float = 0.98,n_pcs: int = None) -> np.ndarray:
  if dim_red:
    pca = PCA(n_components=n_pcs or variance, svd_solver='full')
    X = pca.fit_transform(X)
  C = np.corrcoef(X)
  D = 1.0 - np.abs(C)
  return squareform(pdist(D, metric='euclidean'))
ct0 = list(expr_dict)[0]
D0 = corr2dist(expr_dict[ct0], dim_red=True, n_pcs=10)
print("D0 shape:", D0.shape)

def persist_dgm(D: np.ndarray, maxdim: int = 1):
  return ripser(D, distance_matrix=True, maxdim=maxdim)['dgms']

dgms0 = persist_dgm(D0, maxdim=1)
plot_diagrams(dgms0)

#run 20 mins, running all cells type will take a long time to process data, I decide to pick top 6 cell-type
expr_dict = extract_subpopulations(adata, 'cell_ontology_class')
all_dgms = {}
for ct, X in tqdm(expr_dict.items(), desc="Cell types"):
  D = corr2dist(X, dim_red=True, n_pcs=10)
  dgms = persist_dgm(D, maxdim=1)
  all_dgms[ct] = dgms

X_full = adata.X.A if hasattr(adata.X, "A") else adata.X
adata.obsm['X_pca10'] = PCA(n_components=10, svd_solver="randomized") \
                            .fit_transform(X_full)

top6 = adata.obs['cell_ontology_class'].value_counts().index[:6].tolist()
print("Running PH on:", top6)


def compute_dgms_on_pcs(X_pc, max_samples=2000,maxdim=1, min_persistence=0.1):
  if X_pc.shape[0] > max_samples:
    idx  = np.random.choice(X_pc.shape[0], max_samples,replace=False)
    X_pc = X_pc[idx]
  raw = ripser(X_pc, maxdim=maxdim)['dgms']
  H1  = raw[1]
  keep = np.abs(H1[:,1] - H1[:,0]) >= min_persistence
  return [ raw[0], H1[keep] ]


all_dgms6 = {}
for ct in tqdm(top6, desc="Top-6 PH"):
  mask = (adata.obs['cell_ontology_class'] == ct).values
  X_pc = adata.obsm['X_pca10'][mask]
  all_dgms6[ct] = compute_dgms_on_pcs(X_pc)

types6 = list(all_dgms6)
n6 = len(types6)
B6 = np.zeros((n6, n6))

pairs = list(combinations(range(n6), 2))
for i, j in tqdm(pairs, desc="Bottleneck pairs", total=len(pairs)):
  B6[i, j] = B6[j, i] = bottleneck(all_dgms6[types6[i]][1],all_dgms6[types6[j]][1])

df6 = pd.DataFrame(B6, index=types6, columns=types6).round(2)
print(df6)

link6 = sch.linkage(squareform(B6), method='average')
plt.figure(figsize=(6,4))
sch.dendrogram(link6, labels=types6, leaf_rotation=45)
plt.title("Top-6 cell-type clustering (H₁ bottleneck)")
plt.tight_layout()
plt.show()
#takes 3-4 mins per pair, every unique pair = 6*5/2 = 15 * 3 or 15 * 4 mins = 45-60mins

df6 = pd.DataFrame(B6, index=types6, columns=types6).round(2)
print(df6)

link6 = sch.linkage(squareform(B6), method='average')

plt.figure(figsize=(6,4))
sch.dendrogram(link6, labels=types6, leaf_rotation=45)
plt.title("Top-6 cell-type clustering)")
plt.tight_layout()
plt.show()


meta6 = fcluster(link6, t=0.5, criterion='distance')
cluster_map6 = dict(zip(types6, meta6))
print("Cluster assignment per type:", cluster_map6)


adata.obs['tda_meta6'] = adata.obs['cell_ontology_class'].map(cluster_map6)

sc.pl.umap(adata, color='tda_meta6', legend_loc='on data', title='TDA meta-clusters (6 types)')

def count_bars_at_threshold(H1s, thresholds=[0.0, 0.05, 0.05, 0.12, 0.13]):
    data = {}
    for t in thresholds:
        counts = []
        for ct in top6:
            dgm = H1s[ct]
            n_kept = np.sum((dgm[:,1] - dgm[:,0]) >= t)
            counts.append(n_kept)
        data[t] = counts

    df = pd.DataFrame(data, index=top6)
    return df


H1s = {}
for ct in top6:
  H1s[ct] = all_dgms6[ct][1]
thresholds = [0.0, 0.05, 0.1, 0.2, 0.3]
df_counts = count_bars_at_threshold(H1s, thresholds)
print(df_counts)

min_persistence = 0.1

H1s_filt = {}
for ct in top6:
  mask = adata.obs['cell_ontology_class'] == ct
  X_pc = adata.obsm['X_pca'][mask, :10]
  raw = ripser(X_pc, maxdim=1)['dgms']
  H1 = raw[1]
  keep = (H1[:,1] - H1[:,0]) >= min_persistence
  H1s_filt[ct] = H1[keep]


pimgr = PersistenceImager(pixel_size=0.1)
pimgr.fit(list(H1s_filt.values()))

imgs = np.stack([pimgr.transform(H1s_filt[ct]).ravel() for ct in top6 ])

B = squareform(pdist(imgs, metric='euclidean'))
link = linkage(squareform(B), method='average')

plt.figure(figsize=(6,4))
dendrogram(link, labels=top6, leaf_rotation=45)
plt.title(f"Top-6 clustering with persistence > 0.1")
plt.tight_layout()
plt.show()